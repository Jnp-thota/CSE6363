{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f63cbc02",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "This notebook demonstrates how linear regression is used to model data and predict the values of novel samples.\n",
    "\n",
    "We start with a simple dataset generated from a linear function $y = mx + b$. If we generate values of $x$ linearly, the resulting dataset will be too clean and uninteresting to model. We will simulate some variance here by adding some Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505fd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a bias term and slope between -1 and 1\n",
    "b = (np.random.rand() * 2) - 1\n",
    "m = (np.random.rand() * 2) - 1\n",
    "noise_weight = 0.1\n",
    "\n",
    "num_samples = 100\n",
    "x = np.array(list(range(num_samples)), dtype=np.float32)\n",
    "x = x / num_samples\n",
    "\n",
    "y = m * x + b + (noise_weight * np.random.randn(num_samples))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "# ax.axline([0, b], slope=m, c='r')\n",
    "ax.set_title(\"Original Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d32d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_est = -0.7\n",
    "b_est = 0\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.axline([0, b_est], slope=m_est, c='r')\n",
    "ax.set_title(\"Manually Tuned Model\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce0ecb02",
   "metadata": {},
   "source": [
    "The generated data is plotted above along with the underlying true function that was used to generate it. If we already know what the true function is, our job is done. Suppose that we only have the data points (in blue). How do we go about modelling it? It is reasonable to first visualize the data and observe that it does follow a linear pattern. Thus, a linear model would be a decent model to choose.\n",
    "\n",
    "If the data followed a curve, we may decide to fit a polynomial. We will look at an example of that later on. For now, let's formalize all of the information that we have.\n",
    "\n",
    "- $(\\mathbf{x}, \\mathbf{y})$ - Data points from the original dataset. Generally, $\\mathbf{x}$ is a vector of features and $\\mathbf{y}$ is the target vector. In our simple dataset above, these are both scalar values.\n",
    "- $\\mathbf{w} = (w_0, w_1)$ - Our model parameters. Comparing to the equation $y = mx + b$, $w_0$ is our bias term and $w_1$ is our slope parameter.\n",
    "\n",
    "## Making Predictions\n",
    "\n",
    "Given $\\mathbf{w}$, we can make a prediction for a new data sample $\\mathbf{x} = x_1$.\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}; \\mathbf{w}) = w_0 + w_1 x_1\n",
    "$$\n",
    "\n",
    "Note that the bias term is always added to the result. We can simplify this into a more general form by appending a constant 1 (s.t. $x_0 = 1$) to each of our samples such that $\\mathbf{x} = (1, x_1, ..., x_d)$. Then, the general linear model becomes\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}; \\mathbf{w}) = \\sum_{i=0}^{d} w_i x_i = \\mathbf{w}^T \\mathbf{x}.\n",
    "$$\n",
    "\n",
    "If our data happened to have more than 1 feature, it would be easy enough to model it appropriately using this notation.\n",
    "\n",
    "## Determining Fitness\n",
    "\n",
    "If we really wanted to, we could fit our model by plotting it and manually adjusting the weights until our model looked acceptable by some qualitative standard. Fortunately we won't be doing that. Instead, we will use a quantitative measurement that provides a metric of how well our current parameters fit the data.\n",
    "\n",
    "For this, we use a **cost function** or **loss function**. The most common one to use for this type of model is the least-squares function:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{2}\\sum_{i=1}^{n}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2.\n",
    "$$\n",
    "\n",
    "Let's jump back into the code for a minute and see this function in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a10ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lst_sq(predictions, target):\n",
    "    return 0.5 * np.sum((predictions - target)**2)\n",
    "\n",
    "# Create our parameters and append the bias terms to each data sample in the original dataset.\n",
    "w = np.random.uniform(-1, 1, size=(2,))\n",
    "data = np.hstack((np.ones((x.shape[0], 1)), x[:, None]))\n",
    "\n",
    "error = lst_sq(data @ w, y)\n",
    "print(\"Error =\", error)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.axline([0, w[0]], slope=w[1], c='r')\n",
    "ax.set_title(\"Random Model Parameters\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e0bd7b30",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "Depending on the random initialization of parameters, our error varies greatly. We can observe that no matter what the chose parameters are, there is no possible way we can achieve an error of 0. The best we can do is minimize this error:\n",
    "\n",
    "$$\n",
    "\\min J(\\mathbf{w})\n",
    "$$\n",
    "\n",
    "For this, we rely on stochastic gradient descent. The basic idea is as follows:\n",
    "1. Begin with an initial guess for $\\mathbf{w}$\n",
    "2. Compare the prediction for sample $\\mathbf{x}_{i}$ with its target $\\mathbf{y}_{i}$.\n",
    "3. Update $\\mathbf{w}$ based on the comparison in part 2.\n",
    "4. Repeat steps 2 and 3 on the dataset until the loss has converged.\n",
    "\n",
    "Steps 1, 3, and 4 are easy enough. What about step 2? How can we possibly know how to modify $\\mathbf{w}$ such that $J(\\mathbf{w})$ will decrease? By computing the gradient $\\frac{d}{d\\mathbf{w}}J(\\mathbf{w})$! How will we know when we have arrived at a minima? When $\\nabla J(\\mathbf{w}) = 0$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{d}{d\\mathbf{w}}J(\\mathbf{w}) &= \\frac{d}{d\\mathbf{w}}\\frac{1}{2}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})^2\\\\\n",
    "&= 2 \\cdot \\frac{1}{2}(h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\cdot \\frac{d}{d\\mathbf{w}} (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i})\\\\\n",
    "&= (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\cdot \\frac{d}{d\\mathbf{w}} (\\mathbf{w}^T \\mathbf{x}_{i} - \\mathbf{y}_{i})\\\\\n",
    "&= (h(\\mathbf{x}_{i};\\mathbf{w}) - \\mathbf{y}_{i}) \\mathbf{x}_{i}\n",
    "\\end{align*}\n",
    "\n",
    "The gradient represents the direction of greatest change for a function evaluated With this gradient, we can use an update rule to modify the previous parameter vector $\\mathbf{w}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{t+1} = \\mathbf{w}_{t} - \\alpha \\sum_{i=1}^{n} (h(\\mathbf{x}_{i};\\mathbf{w}_{t}) - \\mathbf{y}_{i}) \\mathbf{x}_{i}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ is an update hyperparameter that allows us to control how big or small of a step our weights can take with each update. In general, a smaller value will be more likely to get stuck in local minima. However, too large of a value may never converge to any minima.\n",
    "\n",
    "For problems like linear regression and logistic regression, we only have a single minima. So, that's a problem for a different day. For now, let's implement sequential learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5b5ef-e0b9-4cff-8a10-536f73b2c959",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [1, 0.2],\n",
    "    [1, 0.3],\n",
    "    [1, 0.4],\n",
    "    [1, 0.5],\n",
    "    [1, 0.6],\n",
    "    #[1, 0.9]\n",
    "])\n",
    "x = np.array([\n",
    "    0.2, 0.3, 0.4, 0.5, 0.6#, 0.9\n",
    "])\n",
    "y = np.array([\n",
    "    0.2, 0.3, 0.4, 0.5, 0.6#, 0.4\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3e8e0-3765-44d6-a0dd-ec248c84d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression().fit(data, y)\n",
    "b = reg.intercept_\n",
    "m = reg.coef_\n",
    "print(m)\n",
    "\n",
    "y_hat = reg.predict(data)\n",
    "\n",
    "# Visualize the new solution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.axline([0, b], slope=m[1], c='b')\n",
    "for i in range(x.shape[0]):\n",
    "    ax.plot([x[i], x[i]], [y[i], y_hat[i]], c='r', linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(weights, predictions, target, data, alpha=0.001):\n",
    "    diff = predictions - target\n",
    "    update = diff @ data\n",
    "    w_new = w - alpha * update\n",
    "    \n",
    "    return w_new\n",
    "\n",
    "# We already have our initial guess from above.\n",
    "# Compute the model's current predictions and run a single step of SGD.\n",
    "y_hat = data @ w\n",
    "error = lst_sq(y_hat, y)\n",
    "print(\"Error =\", error)\n",
    "\n",
    "# Next, we update the weights based on the loss and data\n",
    "# Try using different values for alpha and see what happens\n",
    "for i in range(100):\n",
    "    w = sgd_step(w, y_hat, y, data, 0.01)\n",
    "    y_hat = data @ w\n",
    "\n",
    "print(w)\n",
    "\n",
    "# Recompute the new  y-values for visualization\n",
    "y_hat = data @ w\n",
    "\n",
    "# Visualize the new solution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.axline([0, w[0]], slope=w[1], c='b')\n",
    "ax.axline([0, b], slope=m[1], c='g')\n",
    "for i in range(5):\n",
    "    ax.plot([x[i], x[i]], [y[i], y_hat[i]], c='r', linewidth=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ca7899f",
   "metadata": {},
   "source": [
    "# Fitting a Curve\n",
    "\n",
    "What if our data does not follow a linear model? Perhaps it follows some pattern that curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a bias term and slope between -1 and 1\n",
    "b = (np.random.rand() * 2) - 1\n",
    "m = (np.random.rand() * 2) - 1\n",
    "noise_weight = 0.1\n",
    "\n",
    "num_samples = 100\n",
    "x = np.array(list(range(num_samples)), dtype=np.float32)\n",
    "x = x / num_samples * 2 * math.pi\n",
    "\n",
    "y = m * np.sin(x) + b + (noise_weight * np.random.randn(num_samples))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, m * np.sin(x) + b, c='r')\n",
    "ax.set_title(\"Original Data\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a51f324",
   "metadata": {},
   "source": [
    "# Linear Basis Function Models\n",
    "\n",
    "Now we need to model a nonlinear function, but the model we chose before is limited by the fact that it is linear. Let's see how well our previous model can fit this data. Since we haven't yet, let's solve it using the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9407cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((np.ones((x.shape[0], 1)), x[:, None]))\n",
    "w = np.linalg.inv(data.T @ data) @ (data.T @ y)\n",
    "\n",
    "# Recompute the new  y-values for visualization\n",
    "y_hat = data @ w\n",
    "error = lst_sq(y_hat, y)\n",
    "print(\"Error =\", error)\n",
    "\n",
    "# Visualize the new solution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.axline([0, w[0]], slope=w[1], c='b')\n",
    "ax.plot(x, m * np.sin(x) + b, c='r')\n",
    "for i in range(num_samples):\n",
    "    ax.plot([x[i], x[i]], [y[i], y_hat[i]], c='r', linewidth=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bb990b2",
   "metadata": {},
   "source": [
    "The approach of linear basis function models uses nonlinear functions to transform the original features. The most obvious choice of functions to use the class of polynomial functions. Our model won't look that much different:\n",
    "\n",
    "$$\n",
    "h(\\mathbf{x}; \\mathbf{w}) = \\sum_{i=0}^{d} w_i \\phi_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Let's start by using a second-order polynomial: $\\phi_1(\\mathbf{x}) = x_1$ and $\\phi_2(\\mathbf{x}) = x_1^2$. Note that $\\phi_0(\\mathbf{x}) = 1$ to preserve the bias term. In order to solve this using normal equations, our design matrix needs to change to accommodate the basis functions:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbf{X} =\n",
    "\\begin{bmatrix}\n",
    "1 & \\phi_1(\\mathbf{x}_1) & \\phi_2(\\mathbf{x}_1)\\\\\n",
    "\\vdots & \\vdots & \\vdots\\\\\n",
    "1 & \\phi_1(\\mathbf{x}_n) & \\phi_2(\\mathbf{x}_n)\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "Let's give it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e7fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((np.ones((x.shape[0], 1)), x[:, None], x[:, None]**2))\n",
    "w = np.linalg.inv(data.T @ data) @ (data.T @ y)\n",
    "\n",
    "# Recompute the new  y-values for visualization\n",
    "y_hat = data @ w\n",
    "error = lst_sq(y_hat, y)\n",
    "print(\"Error =\", error)\n",
    "\n",
    "# Visualize the new solution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, y_hat, c='b')\n",
    "ax.plot(x, m * np.sin(x) + b, c='r')\n",
    "for i in range(num_samples):\n",
    "    ax.plot([x[i], x[i]], [y[i], y_hat[i]], c='r', linewidth=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8008bb0f",
   "metadata": {},
   "source": [
    "The error has only slightly improved. We can see that the model can now attempt to fit a curve, but our data clearly requires more degrees of freedom. Let's scale up to a third-degree polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a25881",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.hstack((np.ones((x.shape[0], 1)), x[:, None], x[:, None]**2, x[:, None]**3))\n",
    "w = np.linalg.inv(data.T @ data) @ (data.T @ y)\n",
    "\n",
    "# Recompute the new  y-values for visualization\n",
    "y_hat = data @ w\n",
    "error = lst_sq(y_hat, y)\n",
    "print(\"Error =\", error)\n",
    "\n",
    "# Visualize the new solution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, y_hat, c='b')\n",
    "ax.plot(x, m * np.sin(x) + b, c='r')\n",
    "for i in range(num_samples):\n",
    "    ax.plot([x[i], x[i]], [y[i], y_hat[i]], c='r', linewidth=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae10b6ab-1c42-45a5-8580-cc3d51fff800",
   "metadata": {},
   "source": [
    "We know that the original data was generated from the $\\sin$ function.\n",
    "Let's choose that as our basis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb38691c-b33c-4550-a1d1-b1e13554237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_hat = np.sin(x)\n",
    "\n",
    "data = np.hstack((np.ones((x.shape[0], 1)), x_hat[:, None]))\n",
    "w = np.linalg.inv(data.T @ data) @ (data.T @ y)\n",
    "\n",
    "# Recompute the new  y-values for visualization\n",
    "y_hat = data @ w\n",
    "error = lst_sq(y_hat, y)\n",
    "print(\"Error =\", error)\n",
    "\n",
    "# Visualize the new solution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(x, y)\n",
    "ax.plot(x, y_hat, c='b')\n",
    "ax.plot(x, m * np.sin(x) + b, c='r')\n",
    "for i in range(num_samples):\n",
    "    ax.plot([x[i], x[i]], [y[i], y_hat[i]], c='r', linewidth=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
